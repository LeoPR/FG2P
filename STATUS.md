# STATUS ‚Äî FG2P Project Overview

**Data**: 2026-02-24 19:00
**Status**: Phase 6A CONCLU√çDA ‚úÖ | Exp103 avaliado, hip√≥tese refutada
**SOTA WER**: **Exp9** (PER 0.58%, WER 4.96%, Acc 95.04%, 9.7M params)
**SOTA PER**: **Exp102** (PER 0.52%, WER 5.79%, Acc 94.21%, 9.7M params)
**Finding Phase 6A**: DA Loss + separadores N√ÉO s√£o aditivos (Exp103: PER 0.53%, WER 5.73%). Trade-off PER/WER dos separadores √© fundamental.

---

## üéØ Miss√£o

Desenvolver modelo G2P (Grapheme-to-Phoneme) SOTA para Portugu√™s Brasileiro usando arquitetura BiLSTM Encoder-Decoder + Attention, com foco em:
1. Minimizar PER (Phoneme Error Rate)
2. M√©tricas linguisticamente graduadas (PanPhon features)
3. Reproducibilidade cient√≠fica total
4. ROI computacional (performance vs params)

---

## üèÜ Achievements

### **SOTA Alcan√ßado** ‚úÖ
- **PER: 0.58%** (Exp9) - Supera LatPhon SOTA (0.86%) em -32%
- **Test set: 28.782 palavras** - 57√ó maior que LatPhon (500 samples)
- **Estatisticamente robusto**: œá¬≤ p=0.678, Cram√©r V=0.004 (excellent split)
- **Competitive internacional**: Perto de DeepPhonemizer IT (0.40%) com 42√ó menos params

### **Descobertas Cient√≠ficas** üî¨

#### 1. Split Ratio Impact (Phase 1)
- **60/10/30 > 70/10/20**: -41% PER improvement
- **Conclus√£o**: Mais dados de teste ‚Üí valida√ß√£o estat√≠stica superior

#### 2. Capacity Ceiling (Phase 1-2)
```
4.3M params:  0.66% PER [baseline]
9.7M params:  0.58% PER [SWEET SPOT ‚úì]
17.2M params: 0.60% PER [diminishing returns]
```
- **Satura√ß√£o em ~0.58% PER** com arquitetura LSTM atual

#### 3. Distance-Aware Loss (Phase 3-4)
- ‚úÖ **Funciona em baseline/intermediate** (4.3M-9.7M params)
- ‚ùå **Falha em high-capacity** (17.2M params)
- **Œª optimal = 0.2** (lambda sweep Exp7: 0.05 < 0.20 < 0.50)
- **Mecanismo**: Regulariza√ß√£o impl√≠cita ‚Üí contraprodutivo em modelos grandes

#### 4. PanPhon Features (Phase 2-3)
- **Neutral vs learned embeddings** em PT-BR (Exp3 ‚âà Exp1)
- **Fixed features prejudicam** (Exp4 pior que Exp3)
- **Conclus√£o**: PT-BR ortografia regular ‚Üí learned embeddings suficientes

#### 5. Error Pattern (consistente todos experimentos)
- **65-70% erros**: Confus√µes vogais m√©dias (…õ‚Üîe, …î‚Üîo)
- **Linguisticamente justificado**: Sem contexto sem√¢ntico, ambiguidade inerente
- **Implica√ß√£o**: 0.58% PER pode ser limite "natural" sem context awareness

---

## üìä Experimentos Completos (15 modelos)

| Exp | PER | WER | Acc | Params | Key Feature | Status |
|-----|-----|-----|-----|--------|-------------|--------|
| **Exp102** | **0.52%** | 5.79% | 94.21% | 9.7M | Intermediate + sep | ‚úÖ **SOTA PER** |
| **Exp103** | 0.53% | 5.73% | 94.27% | 9.7M | Intermediate + sep + DA Œª=0.2 | ‚úÖ Phase 6A (refutada) |
| **Exp9** | 0.58% | **4.96%** | **95.04%** | 9.7M | Intermediate + DA Œª=0.2 | ‚úÖ **SOTA WER+Acc** |
| Exp101 | 0.53% | 5.99% | 94.01% | 4.3M | Baseline + sep | ‚úÖ Sep. diagn√≥stico |
| Exp2 | 0.60% | 4.98% | 95.02% | 17.2M | Extended capacity | ‚úÖ High-cap baseline |
| Exp10 | 0.61% | 5.25% | 94.75% | 17.2M | Extended + DA Œª=0.2 | ‚úÖ Negative ROI |
| Exp6 | 0.63% | 5.35% | 94.65% | 4.3M | Baseline + DA Œª=0.1 | ‚úÖ Budget option |
| Exp5 | 0.63% | 5.38% | 94.62% | 9.7M | Intermediate | ‚úÖ Capacity test |
| Exp8 | 0.65% | 5.62% | 94.38% | 4.3M | PanPhon + DA Œª=0.2 | ‚úÖ Features test |
| Exp1 | 0.66% | 5.65% | 94.35% | 4.3M | Baseline 60/10/30 | ‚úÖ Reference |
| Exp3 | 0.66% | 5.45% | 94.55% | 4.3M | PanPhon trainable | ‚úÖ Linguistic features |
| Exp7 | 0.68-0.73% | varies | varies | 4.3M | Lambda sweep | ‚úÖ Hyperopt |
| Exp4 | 0.71% | 6.02% | 93.98% | 4.3M | PanPhon fixed | ‚úÖ Ablation |
| Exp11 | 0.97% | 7.53% | 92.47% | 4.3M | Baseline + decomposed NFD | ‚úÖ NFD incompat√≠vel |
| Exp0 | 1.12% | 9.37% | 90.63% | 4.3M | Baseline 70/10/20 | ‚úÖ Initial baseline |

**Progresso total**: -54% PER (Exp0 1.12% ‚Üí Exp102 0.52%) | SOTA WER: Exp9 (4.96%) | 17 modelos treinados

---

## üöÄ Phase 6A CONCLU√çDA ‚Äî Sep + DA Loss Combination

### Experimentos Planejados

#### **Exp11**: Baseline + Decomposed NFD
- **Config**: `config_exp11_baseline_decomposed.json`
- **Hip√≥tese**: NFD Unicode (√°‚Üía+¬¥) facilita aprendizado diacritics
- **Compara com**: Exp1 (0.66% PER)
- **Target**: 0.60-0.64% PER
- **Status**: Re-iniciado com `keep_syllable_separators=true`

#### **Exp101**: Baseline + Separadores (controle direto)
- **Config**: `config_exp101_baseline_60split_separators.json`
- **Hip√≥tese**: Separadores mudam PER/WER mesmo com encoding raw
- **Compara com**: Exp1 (baseline raw sem separadores)

#### **Exp12**: PanPhon + Decomposed
- **Config**: `config_exp12_panphon_decomposed.json`
- **Hip√≥tese**: Sinergia features lingu√≠sticas + diacritics expl√≠citos
- **Compara com**: Exp3 (0.66%) e Exp11
- **Target**: 0.58-0.62% PER
- **Critical**: Se ‚â• Exp9 ‚Üí 4.3M params rivalizam 9.7M = OPTIMAL ROI

#### **Exp13**: SOTA + Decomposed (FRONTIER PUSH)
- **Config**: `config_exp13_intermediate_distance_aware_decomposed.json`
- **Hip√≥tese**: NFD + SOTA architecture ‚Üí **NEW ABSOLUTE SOTA**
- **Compara com**: Exp9 (0.58% PER)
- **Target**: **< 0.55% PER** (breakthrough PT-BR G2P)

**Estrat√©gia**: Design fatorial 2√ó2 [raw/decomposed] √ó [learned/PanPhon]

---

## ‚úÖ Hotfixes Recentes

- **BUG 1 (cache collision)**: resolvido com nomes sens√≠veis a `encoding + separadores + split + seed`
- **BUG 2 (separadores de s√≠laba)**: flag opcional `keep_syllable_separators`
	- Default: `false` (compat√≠vel com Exp0-10)
	- Exp11+ ativado para testes com separadores
- **Observa√ß√£o**: revalidar impacto direto via Exp101 (baseline raw + separadores)
- **Relat√≥rio HTML**: ordena√ß√£o generalizada por √≠ndice de experimento (evita ordem lexicogr√°fica `exp1, exp11, exp2`) e sort robusto para colunas num√©ricas (PER/WER/Accuracy e m√©tricas graduadas)

### Impacto nos experimentos j√° feitos (avalia√ß√£o retroativa)
- **Exp0‚ÄìExp10**: **sem impacto em m√©tricas hist√≥ricas** (PER/WER/Acc mantidos).
- Motivo: todos foram treinados antes da flag de separadores e permanecem v√°lidos como baseline hist√≥rico.
- **Cache**: mudan√ßa afeta apenas arquivos de inspe√ß√£o em `data/` (evita sobrescrita), n√£o altera checkpoints j√° salvos em `models/`.
- **Comparabilidade**:
	- Exp11 run antigo (`20260222_161238`): decomposed sem separadores.
	- Exp11 run atual (`20260222_201314`): decomposed com separadores.
	- Esses dois runs devem ser tratados como condi√ß√µes experimentais diferentes.

---

## üìà M√©tricas vs Literatura

| Sistema | Idioma | Test Size | PER | Params | Notas |
|---------|--------|-----------|-----|--------|-------|
| **FG2P Exp9** | PT-BR | **28.8k** | **0.58%** | 9.7M | ‚úÖ SOTA atual |
| LatPhon 2025 | PT-BR | 500 | 0.86% | 7.5M | 57√ó menor test set |
| DeepPhonemizer | IT | ~77k | 0.40% | 229M | Romance lingua similar |
| DeepPhonemizer | EN | 120k | 5.23% | 229M | Ortografia irregular |
| ByT5 Small | 100+ | varies | 8.90% | 299M | Multilingual average |

**Destaques**:
- ‚úÖ Supera LatPhon SOTA (-32% PER)
- ‚úÖ Perto de DeepPhonemizer IT com 23√ó menos params
- ‚úÖ Test set mais robusto estatisticamente

---

## üõ†Ô∏è Infraestrutura & Tools

### Core Pipeline
- ‚úÖ **train.py**: Training loop com early stopping, checkpointing
- ‚úÖ **inference.py**: Batch evaluation + metrics + error analysis
- ‚úÖ **analysis.py**: Training history plots + convergence analysis
- ‚úÖ **report_generator.py**: HTML reports com m√©tricas graduadas
- ‚úÖ **manage_experiments.py**: Experiment tracking + cleanup

### Data & Registry
- ‚úÖ **G2PCorpus**: Stratified splits, caching, statistical validation
- ‚úÖ **FileRegistry**: Timestamped artifacts, metadata tracking
- ‚úÖ **dataset_stats.json**: Reproducible cache (phoneme coverage, chi-square)

### Quality Assurance
- ‚úÖ Reproducibilidade total (seed=42, deterministic)
- ‚úÖ Configs JSON versionados com todos hyperparameters
- ‚úÖ Metadata JSON por modelo (architecture, training, metrics)
- ‚úÖ Git-tracked configs + .gitignore models/results

---

## üìÅ Repository Structure

```
FG2P/
‚îú‚îÄ‚îÄ src/                          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ train.py                  # Training pipeline
‚îÇ   ‚îú‚îÄ‚îÄ inference.py              # Evaluation + metrics
‚îÇ   ‚îú‚îÄ‚îÄ g2p.py                    # Model architecture + corpus
‚îÇ   ‚îú‚îÄ‚îÄ analysis.py               # Training analysis
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                  # Logging, paths, helpers
‚îú‚îÄ‚îÄ config_*.json                 # Experiment configs (reproducible)
‚îú‚îÄ‚îÄ models/                       # Model checkpoints .pt + metadata
‚îú‚îÄ‚îÄ results/                      # Evaluations, predictions, analysis
‚îú‚îÄ‚îÄ data/                         # Cached dataset splits
‚îú‚îÄ‚îÄ dicts/                        # Source dictionary pt-br.tsv
‚îú‚îÄ‚îÄ docs/                         # Documentation (scientific paper structure)
‚îÇ   ‚îú‚îÄ‚îÄ 01_LITERATURA.md
‚îÇ   ‚îú‚îÄ‚îÄ 02_ARQUITETURA.md
‚îÇ   ‚îú‚îÄ‚îÄ 03_IMPLEMENTACAO.md
‚îÇ   ‚îú‚îÄ‚îÄ 04_EXPERIMENTOS.md
‚îÇ   ‚îú‚îÄ‚îÄ 05_BENCHMARKS.md
‚îÇ   ‚îî‚îÄ‚îÄ 06_ANALISE_LINGUISTICA.md
‚îú‚îÄ‚îÄ TODO.md                       # Roadmap + status (source of truth)
‚îú‚îÄ‚îÄ RESUMO_EXPERIMENTOS.md        # Consolidated experiment summary
‚îú‚îÄ‚îÄ EXPERIMENTOS_DECOMPOSED.md    # Phase 5 strategy
‚îî‚îÄ‚îÄ README.md                     # Quick start

```

---

## üéØ Pr√≥ximos Milestones

### Curto Prazo (1-2 semanas)
1. ‚úÖ **Exp11-13 training** (decomposed encoding tests)
2. üìä **Comparative analysis** Exp11-13 vs baselines
3. üéØ **Decision**: Adoptar decomposed como default SE beneficial

### M√©dio Prazo (2-4 semanas)
4. üìù **Paper draft** (estrutura j√° em docs/)
5. üé® **PowerPoint generator** para apresenta√ß√µes cient√≠ficas
6. üî¨ **M√©tricas graduadas avan√ßadas** (eros√£o cumulativa)

### Longo Prazo (backlog)
7. ü§ñ **Transformer architecture** (compare vs LSTM)
8. üåê **Multi-task learning** (tonicidade, syllabification)
9. üì¶ **Production API** (Flask/FastAPI deployment)

---

## üìö Documenta√ß√£o

### Principal
- **[README.md](README.md)**: Quick start + resultados destacados
- **[TODO.md](TODO.md)**: Roadmap completo + tracking
- **[RESUMO_EXPERIMENTOS.md](RESUMO_EXPERIMENTOS.md)**: An√°lise consolidada Exp0-10

### Cient√≠fica (paper structure)
- **[docs/](docs/)**: 6 documentos (Literatura ‚Üí An√°lise lingu√≠stica)
- **[EXPERIMENTOS_DECOMPOSED.md](EXPERIMENTOS_DECOMPOSED.md)**: Phase 5 strategy

### Configuration
- **[config_*.json](./config_exp*.json)**: 13 experiment configs (reproducible)
- **[CONFIG_README.md](CONFIG_README.md)**: Config file format specification

---

## üë• Team & Contact

**Desenvolvedor Principal**: [Nome]  
**Orienta√ß√£o**: [Orientador]  
**Institui√ß√£o**: [Universidade]  
**Curso**: [Programa]

---

**√öltima atualiza√ß√£o**: 2026-02-22 16:40  
**Gerado automaticamente** a partir do estado atual do projeto.
