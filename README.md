# FG2P ‚Äî Convers√£o Grapheme-to-Phoneme para Portugu√™s Brasileiro

Modelo neural BiLSTM Encoder-Decoder + Attention para converter texto PT-BR em transcri√ß√£o fon√©tica IPA. 

**üèÜ SOTA**: **PER 0.58%** (Exp9, 9.7M params) | Exp2: 0.60% (17.2M) | Exp6: 0.63% (4.3M, budget)

**Breakthrough**: Exp9 (Intermediate + Distance-Aware Loss Œª=0.2) alcan√ßa NOVO SOTA PT-BR G2P, superando LatPhon (0.86%) com test set 57√ó maior.

---

## üöÄ Quick Start

```bash
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# Treinar (Exp9 ‚Äî SOTA recomendado)
python src\train.py --config conf/config_exp9_intermediate_distance_aware.json

# Avaliar (full evaluation)
python src\inference.py

# Teste r√°pido com neologismos
python src\inference_light.py --model-index 0 --test data/neologisms_test.tsv

# Validar dataset sa√∫de
python src\dataset_health_check.py --input dicts/pt-br.tsv

# Relat√≥rio HTML
python src\reporting\report_generator.py

# Gest√£o de experimentos
python src\manage_experiments.py --list
python src\manage_experiments.py --prune-incomplete --dry-run
```

---

## üéØ Capacidades Principais

### **1. SOTA G2P Model** (Exp9)
- PER 0.58% | WER 4.96% | Accuracy 95.04%
- 9.7M params (optimal ROI vs capacity)
- BiLSTM Encoder-Decoder + Attention + Distance-Aware Loss
- Production-ready com checkpointing autom√°tico

### **2. Neologisms & OOV Testing** (NEW - Phase 5A) üÜï
- `inference_light.py` ‚Äî Teste r√°pido de palavras novas
- Detec√ß√£o de palavras inventadas vs dicion√°rio
- Confidence score + nearest match suggestions
- Uso: Avaliar performance em nomes, termos t√©cnicos, loanwords

### **3. Dataset Quality Assurance** (NEW - Phase 5A) üÜï
- `dataset_health_check.py` ‚Äî Valida dicion√°rio
- Detecta duplicatas, typos, encoding issues
- HTML report com sugest√µes de corre√ß√£o
- Estat√≠sticas de cobertura (phonemes, n-grams)

### **4. Comprehensive Analysis Pipeline**
- HTML reports com gr√°ficos de converg√™ncia
- M√©tricas graduadas PanPhon (Classes A/B/C/D)
- Error analysis autom√°tico (confus√µes estruturadas)
- Compara√ß√£o multi-modelo com SOTA literatura

---

## üìä Resultados Destacados

| Exp | Params | T√©cnica | PER‚Üì | WER‚Üì | Acc‚Üë | ROI |
|-----|--------|---------|------|------|------|-----|
| **Exp9** | 9.7M | Intermediate + DA Loss Œª=0.2 | **0.58%** | **4.96%** | **95.04%** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **SOTA** |
| **Exp2** | 17.2M | Extended | 0.60% | 4.98% | 95.02% | ‚≠ê‚≠ê‚≠ê High capacity |
| **Exp6** | 4.3M | Baseline + DA Loss Œª=0.1 | 0.63% | 5.35% | 94.65% | ‚≠ê‚≠ê‚≠ê‚≠ê Budget |
| **Exp10** | 17.2M | Extended + DA Loss Œª=0.2 | 0.61% | 5.25% | 94.75% | ‚≠ê Negative ROI |
| **Exp5** | 9.7M | Intermediate | 0.63% | 5.38% | 94.62% | ‚≠ê‚≠ê‚≠ê Sweet spot |
| **Exp1** | 4.3M | Baseline | 0.66% | 5.65% | 94.35% | ‚≠ê‚≠ê‚≠ê Simple |

**Key Insights**: 
- ‚úÖ **Exp9 (9.7M) confirmado como SOTA**: Melhor PER/WER/Acc, optimal ROI
- ‚ùå **DA Loss n√£o escala para high-capacity**: Exp10 (17.2M) pior que Exp2 e Exp9
- üí° **Satura√ß√£o em ~0.58% PER**: Limite alcan√ßado com arquitetura atual
- üéØ **Pr√≥xima fronteira**: Decomposed encoding (Exp11-13) para superar 0.58%

**An√°lise detalhada**: [docs/04_EXPERIMENTS.md](docs/04_EXPERIMENTS.md)

---

## üìö Documenta√ß√£o (Estrutura de Artigo Cient√≠fico)

**Leitura recomendada em ordem**:

1. **[docs/01_OVERVIEW.md](docs/01_OVERVIEW.md)** ‚Äî Introdu√ß√£o, dataset, discovery 60/10/30
2. **[docs/02_ARCHITECTURE.md](docs/02_ARCHITECTURE.md)** ‚Äî BiLSTM, Attention, Embeddings, tratamento sequ√™ncias
3. **[docs/03_METRICS.md](docs/03_METRICS.md)** ‚Äî PER, WER, m√©tricas graduadas PanPhon (Classes A/B/C/D)
4. **[docs/04_EXPERIMENTS.md](docs/04_EXPERIMENTS.md)** ‚Äî Exp0-9 design, resultados, RFC_EXP6, an√°lise comparativa
5. **[docs/05_THEORY.md](docs/05_THEORY.md)** ‚Äî Funda√ß√µes G2P, Loss functions, Features articulat√≥rias
6. **[docs/06_REFERENCES.md](docs/06_REFERENCES.md)** ‚Äî Bibliography (SOTA, datasets, tools)

**Status & Roadmap**: [TODO.md](TODO.md) ‚Äî Fonte √∫nica de status, Phase 3 schedule

**Benchmarks**: [docs/performance.json](docs/performance.json) ‚Äî SOTA comparisons + hyperparameters
