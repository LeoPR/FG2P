# FG2P â€” ConversÃ£o Grafema-para-Fonema em PortuguÃªs Brasileiro

> **Escopo**: Modelo neuronal BiLSTM Encoder-Decoder com atenÃ§Ã£o Bahdanau para conversÃ£o automÃ¡tica de texto (grafemas) em transcriÃ§Ã£o fonÃ©tica IPA (fonemas) para o PortuguÃªs Brasileiro.

**Data Ãºltima atualizaÃ§Ã£o**: 2026-02-28
**Status**: âœ… Phase 6C Completa | DocumentaÃ§Ã£o consolidada em 8 arquivos
**CÃ³digo-fonte**: [src/](../src/) | **Dados**: [data/](../data/) | **Modelos**: [models/](../models/) | **Resultados**: [results/](../results/)

---

## ğŸ¯ Objetivo

Construir um modelo de alta precisÃ£o que converta palavras do PortuguÃªs Brasileiro em sua representaÃ§Ã£o fonÃ©tica IPA, com aplicaÃ§Ãµes em:
- **SÃ­ntese de fala (Text-to-Speech)**: "casa" â†’ [k-a-z-a]
- **Pesquisa linguÃ­stica**: AnÃ¡lise de padrÃµes fonolÃ³gicos PT-BR
- **Processamento de linguagem natural**: ExtraÃ§Ã£o de features fonÃ©ticas

**MÃ©trica focal**: **PER (Phoneme Error Rate)** minimizado; secundÃ¡ria: **WER (Word Error Rate)**.

---

## ğŸ“Š Resultados Principais

| **Exp** | **Params** | **Loss** | **Sep** | **PER â†“** | **WER â†“** | **Acc â†‘** | **Nota** |
|---------|-----------|----------|---------|-----------|-----------|-----------|----------|
| Exp5 | 9,7M | CE | nÃ£o | 0,63% | 5,38% | 94,62% | Baseline intermediÃ¡rio |
| **Exp9** | 9,7M | DA Î»=0,2 | nÃ£o | 0,58% | **4,96%** | **95,04%** | **SOTA WER** |
| Exp102 | 9,7M | CE | sim | 0,52% | 5,79% | 94,21% | Sep baseline |
| **Exp104b** | 9,7M | DA Î»=0,2 + dist | sim | **0,49%** | 5,43% | 94,57% | **SOTA PER** |
| Exp105 | 9,7M | DA Î»=0,2 + dist | sim | 0,54% | 5,87% | 94,13% | 50% dados â€” robustez |
| Exp106 | 9,7M | DA Î»=0,2 + dist | sim | 0,58% | 6,12% | 93,88% | Sem hÃ­fen â€” 2,58Ã— speed âš¡ |

**Descobertas-chave**:
- **SOTA WER**: Exp9 (4,96%) â€” DA Loss sem separadores
- **SOTA PER**: Exp104b (0,49%) â€” DA Loss + separadores + override de distÃ¢ncia
- **Trade-off PER/WER**: Separadores melhoram PER mas impactam WER â€” trade-off Pareto fundamental
- **AblaÃ§Ãµes**: 50% dados â†’ +0,05% PER (robusto); sem hÃ­fen â†’ +0,04% PER, 2,58Ã— speed

---

## ğŸ“š Estrutura de DocumentaÃ§Ã£o (8 arquivos)

| Arquivo | PropÃ³sito | Tamanho |
|---------|-----------|---------|
| **[01_OVERVIEW.md](01_OVERVIEW.md)** | Este Ã­ndice de navegaÃ§Ã£o | ~150 linhas |
| **[16_SCIENTIFIC_ARTICLE.md](16_SCIENTIFIC_ARTICLE.md)** | Artigo cientÃ­fico completo | ~950+ linhas |
| **[04_EXPERIMENTS.md](04_EXPERIMENTS.md)** | Log completo de todos os experimentos | ~929 linhas |
| **[10_REFERENCES.md](10_REFERENCES.md)** | Bibliografia canÃ´nica | ~844 linhas |
| **[12_DATA_PIPELINE.md](12_DATA_PIPELINE.md)** | Pipeline tÃ©cnico: corpus â†’ vocabulÃ¡rios â†’ treino | ~500 linhas |
| **[GLOSSARIO.md](GLOSSARIO.md)** | GlossÃ¡rio Ãºnico: fonÃ©tica, ML, termos do projeto | ~500 linhas |
| **[17_APRESENTACAO.md](17_APRESENTACAO.md)** | Fonte dos slides PPTX â€” **INTOCÃVEL** | ~272 linhas |
| **[REFACTORING_PRESENTATION_GENERATOR.md](REFACTORING_PRESENTATION_GENERATOR.md)** | Docs do gerador PPTX | ~192 linhas |

---

## ğŸš€ Quick Start

```bash
# Setup
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# InferÃªncia com modelo SOTA PER (Exp104b, index=18)
python src/inference_light.py --index 18 --word computador
# â†’ k Ãµ p u t a . Ëˆ d o x .

# AvaliaÃ§Ã£o completa
python src/inference.py

# RelatÃ³rio HTML
python src/reporting/report_generator.py

# ApresentaÃ§Ã£o PPTX
python src/reporting/presentation_generator.py --compact   # â†’ results/fg2p_presentation.pptx
```

---

## ğŸ“– Leitura Recomendada

**VisÃ£o geral rÃ¡pida** (~15 min):
â†’ Este arquivo + [16_SCIENTIFIC_ARTICLE.md](16_SCIENTIFIC_ARTICLE.md) (SeÃ§Ãµes 1, 5, 9)

**Artigo completo** (~2h):
â†’ [16_SCIENTIFIC_ARTICLE.md](16_SCIENTIFIC_ARTICLE.md) â€” leitura linear, IMRaD

**Log de experimentos** (referÃªncia):
â†’ [04_EXPERIMENTS.md](04_EXPERIMENTS.md) â€” tabela completa Exp0â€“106, anÃ¡lise detalhada

**Pipeline tÃ©cnico** (implementaÃ§Ã£o):
â†’ [12_DATA_PIPELINE.md](12_DATA_PIPELINE.md) â€” corpus â†’ transformaÃ§Ãµes â†’ vocabulÃ¡rios â†’ splits

**Termos e definiÃ§Ãµes**:
â†’ [GLOSSARIO.md](GLOSSARIO.md) â€” fonÃ©tica PT-BR, ML, termos do projeto

---

## ğŸ—ï¸ Arquitetura

**BiLSTM Encoder-Decoder + AtenÃ§Ã£o Bahdanau**

```
Grafemas ("casa")
    â†“
[Embedding 192D aprendido]
    â†“
[BiLSTM Encoder 2 camadas, 384D hidden]
    â†“
[AtenÃ§Ã£o de Bahdanau]
    â†“
[LSTM Decoder 2 camadas]
    â†“
[ProjeÃ§Ã£o Linear â†’ Softmax]
    â†“
Fonemas IPA [k-a-z-a]
```

**Detalhes completos**: [16_SCIENTIFIC_ARTICLE.md Â§ 3](16_SCIENTIFIC_ARTICLE.md) â€” SeÃ§Ã£o 3 (Arquitetura)

---

## ğŸ“ Estrutura de Arquivos

```
FG2P/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ conf/                        â† ConfiguraÃ§Ãµes (Exp0â€“Exp106)
â”‚   â””â”€â”€ config_exp106_no_hyphen_50split.json (e 25 mais)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ 01_OVERVIEW.md           â† Ãndice (vocÃª estÃ¡ aqui)
â”‚   â”œâ”€â”€ 04_EXPERIMENTS.md        â† Log completo de experimentos
â”‚   â”œâ”€â”€ 10_REFERENCES.md         â† Bibliografia canÃ´nica
â”‚   â”œâ”€â”€ 12_DATA_PIPELINE.md      â† Pipeline de dados
â”‚   â”œâ”€â”€ 16_SCIENTIFIC_ARTICLE.md â† Artigo cientÃ­fico completo
â”‚   â”œâ”€â”€ 17_APRESENTACAO.md       â† Fonte dos slides PPTX (INTOCÃVEL)
â”‚   â”œâ”€â”€ GLOSSARIO.md             â† GlossÃ¡rio Ãºnico
â”‚   â”œâ”€â”€ REFACTORING_PRESENTATION_GENERATOR.md â† Docs do gerador
â”‚   â”‚
â”‚   â”œâ”€â”€ generalization_test.tsv  â† 31 palavras OOV curadas
â”‚   â”œâ”€â”€ neologisms_test.tsv      â† Neologismos PT-BR
â”‚   â”œâ”€â”€ performance.json         â† Benchmarks + comparaÃ§Ã£o SOTA
â”‚   â””â”€â”€ REFERENCIAS.bib          â† BibTeX (suporte LaTeX/Word)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ g2p.py                   â† Dataset, CharVocab, PhonemeVocab, modelo
â”‚   â”œâ”€â”€ train.py                 â† Loop de treino + early stopping
â”‚   â”œâ”€â”€ inference.py             â† AvaliaÃ§Ã£o WER/PER sobre test set
â”‚   â”œâ”€â”€ inference_light.py       â† G2PPredictor API mÃ­nima (produÃ§Ã£o/CLI)
â”‚   â”œâ”€â”€ utils.py                 â† get_all_models_sorted(), CHAR_MAPPING
â”‚   â”œâ”€â”€ losses.py                â† CrossEntropyLoss + DistanceAwareLoss
â”‚   â””â”€â”€ reporting/
â”‚       â”œâ”€â”€ report_generator.py  â† RelatÃ³rio HTML
â”‚       â””â”€â”€ presentation_generator.py â† Gerador PPTX (lÃª 17_APRESENTACAO.md)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.txt / val.txt / test.txt
â”‚   â””â”€â”€ dataset_stats.json
â”‚
â””â”€â”€ models/
    â””â”€â”€ *.pt + *_metadata.json
```

---

## ğŸ”§ Qual modelo usar

| Modelo | Index | PER | WER | Quando usar |
|--------|-------|-----|-----|-------------|
| **Exp9** | 11 | 0,58% | **4,96%** | PrecisÃ£o por palavra â€” NLP, lookup, TTS quando WER importa |
| **Exp104b** | 18 | **0,49%** | 5,43% | PrecisÃ£o por fonema â€” anÃ¡lise linguÃ­stica, sÃ­ntese de fala |
| Exp106 | 20 | 0,58% | 6,12% | LatÃªncia crÃ­tica â€” 30,2 w/s (2,58Ã— mais rÃ¡pido) |

---

**Ãšltima atualizaÃ§Ã£o**: 2026-02-28
**Status**: Phase 6C concluÃ­da â€” SOTA PER 0,49% (Exp104b) | DocumentaÃ§Ã£o consolidada (25â†’8 arquivos)
