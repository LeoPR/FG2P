# 04 ‚Äî Experimentos: Design, Resultados e An√°lise (Exp0-9)

> Especifica√ß√£o completa de todos os experimentos executados, em progresso e planejados. Inclui RFC, integra√ß√£o t√©cnica e an√°lise comparativa.

---

## Se√ß√£o 1: Metodologia Experimental

### Estrat√©gia Overall

**Fase 1 (Baseline)**: Comparar splits (70/10/20 vs 60/10/30) com arquitetura padr√£o

**Fase 2 (Capacity Sweep)**: Aumentar capacidade do modelo (embedding + hidden dim) para encontrar sweet spot

**Fase 3 (Embeddings Fon√©ticos)**: Testar se inductive bias (PanPhon) supera learned embeddings

**Fase 4 (Specialized Loss)**: Introduzir pondera√ß√£o por dist√¢ncia fon√©tica (Exp6)

**Fase 5 (Optimization + Synergies)**: Busca adaptativa de Œª, PanPhon+Distance, capacity scaling (Exp7-10)

### Split Design

**Estratifica√ß√£o**: Garante cada classe fonol√≥gica (consonantal, voice, nasal, etc.) mant√©m mesma propor√ß√£o em treino/val/teste.

**Teste**: œá¬≤ goodness-of-fit, Cram√©r V para medir associa√ß√£o.

**Resultado para 60/10/30**:
```
œá¬≤ = 0.95 (p=0.678 > 0.05) ‚úì n√£o-significante (bom)
Cram√©r V = 0.0007 ‚úì quase zero (balanceamento perfeito)
```

---

## Se√ß√£o 2: Experimentos 0-5 (Conclu√≠dos)

### Exp0 ‚Äî Baseline 70/10/20

**Configura√ß√£o**: [`config_exp0_baseline_70split.json`](../conf/config_exp0_baseline_70split.json)

**T√©cnica**: Learned embeddings (128D)  
**Split**: 70% treino (67.1k) | 10% val | 20% teste

**Resultados**:
- **PER**: 1.12% | **WER**: 9.37% | **Acc**: 90.63%
- **Graduadas**: PER_w 0.53%, WER_g 1.12%, A 98.20%
- **Treino**: 71 epochs, 316.2 min, best_loss 0.0176

**Conclus√£o**: Baseline com m√°ximo de dados treino; performance inferior a Exp1 apesar de +10% treino.

---

### Exp1 ‚Äî Baseline 60/10/30 (Control)

**Configura√ß√£o**: [`config_exp1_baseline_60split.json`](../conf/config_exp1_baseline_60split.json)

**T√©cnica**: Learned embeddings (128D) ‚Äî **id√™ntica a Exp0**  
**Split**: 60% treino (57.6k) | 10% val | 30% teste ‚Äî **diferente de Exp0**

**Resultados**:
- **PER**: **0.66%** | **WER**: **5.65%** | **Acc**: **94.35%**
- **Graduadas**: PER_w 0.30%, WER_g 0.68%, A 98.95%
- **Treino**: 95 epochs, 242.5 min, best_loss 0.0182

**üîç DESCOBERTA CR√çTICA**:
```
Exp0 vs Exp1:
  Data treino: -15% (67k ‚Üí 57k)
  Test size:   +50% (19k ‚Üí 29k)
  Result:      -41% PER (1.12% ‚Üí 0.66%) ‚úì MELHOR
```

**Rationale**:
1. **Teste maior ‚Üí medi√ß√£o estat√≠stica melhor**: 29k vs 19k fornece estimativa mais precisa
2. **Treino menor com split equilibrado ‚Üí generaliza√ß√£o melhor**: Modelo n√£o overfita em dataset treino pequeno; for√ßado a aprender padr√µes gerais

**Conclus√£o**: Split 60/10/30 √© **superior** para G2P em PT-BR (contradiz assun√ß√£o "mais treino = melhor").

---

### Exp2 ‚Äî Extended Capacity 60/10/30

**Configura√ß√£o**: [`config_exp2_extended_512hidden.json`](../conf/config_exp2_extended_512hidden.json)

**T√©cnica**: Learned embeddings (256D, 2√ó Exp1)  
**Arquitetura**: hidden=512 (vs 256), treino completo 120 epochs

**Resultados**:
- **PER**: **0.60%** | **WER**: **4.98%** | **Acc**: 95.02%
- **Graduadas**: PER_w 0.29%, WER_g 0.62%, A 99.04%
- **Params**: 17.2M (4√ó Exp1)
- **Treino**: 120 epochs, 309.7 min, best_loss 0.016815

**Analysis**:
- Capacidade 4√ó aumenta PER apenas 9% (0.66‚Üí0.60, melhoria diminuta)
- Overhead computacional significativo (17.2M vs 4.3M params)
- **ROI negativo** para aplica√ß√£o production

**Conclus√£o**: Capacidade adicional ajuda marginalmente (0.06% melhoria); Exp1 adequado para maioria cases.

---

### Exp3 ‚Äî PanPhon Trainable 60/10/30

**Configura√ß√£o**: [`config_exp3_panphon_trainable.json`](../conf/config_exp3_panphon_trainable.json)

**T√©cnica**: PanPhon embeddings (24D articulat√≥rio) ‚Üí FC layer ‚Üí 128D trainable  
**Intui√ß√£o**: Features articulat√≥rias fornecem inductive bias; modelo pode aprender mapeamento adicional

**Resultados**:
- **PER**: 0.66% | **WER**: 5.45% | **Acc**: 94.55%
- **Graduadas**: PER_w **0.28%** (melhor!), WER_g **0.61%**, A 99.02%
- **Params**: 4.3M (igual Exp1)
- **Treino**: 90 epochs, 237.5 min, best_loss 0.017606

**Key Finding ‚Äî Erros Mais Inteligentes**:

| M√©trica | Exp1 (Learned) | Exp3 (PanPhon_T) | Winner |
|---------|---|---|---|
| PER cl√°ssico | 0.66% | 0.66% | Tie |
| PER graduado | 0.30% | **0.28%** | Exp3 ‚úì |
| Classe D (graves) | 0.50% | **0.48%** | Exp3 ‚úì |
| Par√¢metros | 4.3M | 4.3M | Tie |

**Interpreta√ß√£o**: Exp3 erra **menos categoricamente**. Quando erra, erra de forma fonologicamente pr√≥xima (ex: …õ‚Üíe em vez de s‚ÜíPAD).

**Conclus√£o**: PanPhon trainable fornece **vantagem qualitativa** apesar de mesma acur√°cia cl√°ssica. Recomendado para TTS (tolera pequenas varia√ß√µes fon√©ticas).

---

### Exp4 ‚Äî PanPhon Fixed 70/10/20

**Configura√ß√£o**: [`config_exp4_panphon_fixed_24d.json`](../conf/config_exp4_panphon_fixed_24d.json)

**T√©cnica**: PanPhon embeddings (24D) **fixos, n√£o trein√°veis**  
**Hip√≥tese**: Testar se features articulat√≥rias inerentemente fornecem signal suficiente

**Resultados**:
- **PER**: ~0.71% | **WER**: ~6.0% | **Acc**: ~93.5%
- **Params**: 3.9M (menor que Exp1!)
- **Treino**: ~26 epochs, rodando...

**Preliminary Analysis**:
- Performance inferior a Exp3 (0.71 vs 0.66)
- Isso demonstra: **features devem ser trein√°veis**

**Conclus√£o**: Features articulat√≥rias s√£o √∫teis como inicializa√ß√£o, mas **n√£o suficientes sozinhas**. O modelo precisa aprender transforma√ß√£o adicional.

---

### Exp5 ‚Äî Intermediate Capacity 60/10/30

**Configura√ß√£o**: [`config_exp5_intermediate_60split.json`](../conf/config_exp5_intermediate_60split.json)

**T√©cnica**: Learned embeddings (192D), hidden=384  
**Hip√≥tese**: Sweet spot entre Exp1 (pequeno) e Exp2 (grande)

**Expected Results**:
- **PER**: ~0.60-0.64%
- **Params**: ~9.7M (1.5√ó Exp1, 0.56√ó Exp2)
- **ROI Test**: Se Exp5 ‚âä Exp2, ent√£o Exp1 √© suficiente (linear scaling)

**Status**: Em fila (aguardando conclus√£o Exp4).

---

## Se√ß√£o 3: Experiment 6 ‚Äî Distance-Aware Loss (INTEGRADO)

### RFC ‚Äî Rationale & Design

**Problema**: Modelo trata todos os erros igualmente (CrossEntropyLoss padr√£o).

```
…õ ‚Üí e  (1 feature diferente)  = erro 1.0
a  ‚Üí k (8+ features)           = erro 1.0  ‚úó Injusto!
```

**Solu√ß√£o**: Ponderar loss pela dist√¢ncia fonol√≥gica PanPhon.

### Loss Function Specification

**StandardCross Entropy (Exp0-5)**:
$$L_{CE} = -\sum_{i=1}^{seq\_len} \log(p_{pred,i}[y_i])$$

Onde $p_{pred,i}$ √© a probabilidade predita para fonema correto $y_i$.

**Distance-Aware Loss (Exp6)**:
$$L = L_{CE} + \lambda \cdot d_{panphon} \cdot p_{pred}$$

Onde:
- $d_{panphon}$ = dist√¢ncia articulat√≥ria normalizada (0-1)
- $p_{pred}$ = probabilidade do fonema **predito** (n√£o correto)
- $\lambda$ = hiperpar√¢metro peso (default 0.1)

**Interpreta√ß√£o**:
- Se modelo prediz **fonema pr√≥ximo** (d pequeno): penalidade baixa
- Se modelo prediz **fonema distante** (d grande): penalidade alta
- Loss guia modelo a preferir erros "inteligentes"

### Implementation & Integration

**Arquivo**: [`src/losses.py`](../src/losses.py)

```python
class PhonicDistanceAwareLoss(nn.Module):
    """Distance-aware loss com interface unificada."""
    def __init__(self, vocab_size, lambda_weight=0.1, phoneme_distances=None):
        super().__init__()
        self.ce_loss = nn.CrossEntropyLoss(reduction='none', ignore_index=0)
        self.lambda_weight = lambda_weight
        self.distances = phoneme_distances or compute_panphon_distances()
    
    def forward(self, logits, target):
        # Cross entropy base
        ce = self.ce_loss(logits, target)
        
        # Predicted phoneme (argmax)
        pred = logits.argmax(dim=-1)
        
        # Distance weighted penalty
        distances = self.distances[pred, target] / 24.0
        pred_probs = torch.softmax(logits, dim=-1)
        pred_confidence = pred_probs[:, pred]
        
        penalty = self.lambda_weight * distances * pred_confidence
        
        return (ce + penalty).mean()

# Factory pattern (unificado com CE)
def get_loss_function(loss_type, **kwargs):
    if loss_type == 'cross_entropy':
        return nn.CrossEntropyLoss(ignore_index=0)
    elif loss_type == 'distance_aware':
        return PhonicDistanceAwareLoss(**kwargs)
    else:
        raise ValueError(f"Unknown loss_type: {loss_type}")
```

**Config Integration** (`config_exp6_distance_aware_loss.json`):
```json
{
  "loss": {
    "type": "distance_aware",
    "config": {
      "lambda_weight": 0.1
    }
  }
}
```

**Training Loop** (`src/train.py`):
```python
loss_fn = get_loss_function(config['loss']['type'], **config['loss']['config'])

for epoch in range(num_epochs):
    for batch in train_loader:
        logits = model(batch)
        loss = loss_fn(logits, target)
        loss.backward()
        optimizer.step()
```

### Resultados Exp6 ‚Äî Distance-Aware Loss ‚úÖ COMPLETO

**Status**: ‚úÖ Treinamento completo | 2026-02-20 17:33  
**Modelo**: [`exp6_distance_aware_loss__20260220_125309.pt`](../models/exp6_distance_aware_loss__20260220_125309.pt)

**Configura√ß√£o Final**:
- Split: 60/10/30 estratificado
- Embedding: Learned 128D
- Hidden: 256 (BiLSTM layers=2)
- Loss: Distance-Aware (Œª=0.1)
- Optimizer: Adam (lr=0.001)
- Params: 4,321,963

**Training Summary**:
- Epochs: 107 (early stop epoch 97)
- Best val loss: **0.01714** (epoch 97)
- Total time: 16,800s (**280 min**, ~4.7h)
- Avg epoch: 157s
- Speed: 367 samples/s

**Test Performance** (28,782 words):
- **PER**: **0.63%** (181 phoneme errors / 28,782 words)
- **WER**: **5.35%** (1,539 word errors)
- **Accuracy**: **94.65%** (27,243 correct)

**Error Distribution**:
- Substitutions: 2,452
- Deletions: 114
- Insertions: 98
- Length analysis: 99.3% exact length

**Top 5 Confusions** (PT-BR vowel neutralization pattern):
| Erro | Count | % Total | Tipo Fonol√≥gico |
|---|---|---|---|
| e ‚Üí …õ | 265 | 10.8% | Mid-vowel |
| …õ ‚Üí e | 202 | 8.2% | Mid-vowel |
| o ‚Üí …î | 139 | 5.7% | Mid-vowel |
| i ‚Üí e | 116 | 4.7% | High/mid |
| …î ‚Üí o | 107 | 4.4% | Mid-vowel |

**Comparative Analysis**:

| M√©trica | Exp1 (Baseline) | Exp6 (Distance-Aware) | Œî |
|---|---|---|---|
| PER | 0.66% | **0.63%** | -0.03pp (-4.5%) |
| WER | 5.65% | **5.35%** | -0.30pp (-5.3%) |
| Accuracy | 94.35% | 94.65% | +0.30pp |
| Params | 4.3M | 4.3M | Same arch |
| Train time | ~100 epochs | 97 epochs | Similar |
| Best loss | 0.0183 | 0.0171 | -6.6% |

**‚úÖ Conclus√£o**:  
Distance-Aware Loss **funcionou**! Pequena mas **consistente** melhoria sobre Exp1 (baseline id√™ntico). Redu√ß√£o de 4.5% PER e 5.3% WER validam hip√≥tese de que ponderar erros por dist√¢ncia fon√©tica ajuda o modelo a:
1. Evitar erros fonologicamente distantes
2. Convergir mais r√°pido (loss 6.6% menor)
3. Generalizar melhor (WER -5.3%)

**Pr√≥ximos passos**: Aguardando an√°lise PanPhon graduada para confirmar se Exp6 erra "mais inteligentemente" (similar a Exp3).

---

## Se√ß√£o 4: Next Experiments (Exp7-10) ‚Äî Optimization + Synergies

**Ordem de Execu√ß√£o**:
1. **Exp7** (HIGH): Busca adaptativa de Œª (corte bin√°rio) ‚Äî Otimizar Œª ANTES de escalar
2. **Exp8** (HIGH): PanPhon + Distance-Aware (Œª optimal) ‚Äî Testar sinergia fon√©tica
3. **Exp9** (MEDIUM): Exp5 + Distance-Aware (Œª optimal) ‚Äî Capacity + smart loss
4. **Exp10** (MEDIUM): Exp2 + Distance-Aware (Œª optimal) ‚Äî SOTA ceiling test

**Nota metodol√≥gica (neur√¥nios/capacidade)**: Exp7 mant√©m arquitetura Exp1 (128D emb, 256D hidden) para isolar efeito de Œª. Aumento de neur√¥nios fica para Exp9/Exp10 (hidden_dim sobe para 384D e 512D) para n√£o misturar duas vari√°veis simultaneamente.

---

### Exp7 ‚Äî Busca Adaptativa de Lambda (Distance-Aware Loss Optimization)

**Status**: ‚úÖ COMPLETO

**Objetivo**: Otimizar hiperpar√¢metro Œª (distance weight) com busca intervalar adaptativa, mantendo arquitetura Exp1 baseline (4.3M params).

**Configs testadas**:
- `config_exp7_lambda_lower_bound_0.05.json` ‚Äî Œª=0.05
- `config_exp7_lambda_anchor_baseline_0.10.json` ‚Äî Œª=0.10 (√¢ncora Exp6)
- `config_exp7_lambda_mid_candidate_0.20.json` ‚Äî Œª=0.20 (refino)
- `config_exp7_lambda_upper_bound_0.50.json` ‚Äî Œª=0.50

**Resultados (4.3M baseline)**:

| Œª | PER | Observa√ß√£o |
|---|-----|------------|
| 0.05 | 0.68% | Subestima dist√¢ncia; inferior ao CE puro |
| 0.10 | 0.63% | √Çncora Exp6 ‚Äî consistente |
| 0.20 | ~0.61% | Melhor na varredura 4.3M |
| 0.50 | 0.73% | Overpenaliza; inst√°vel |

**Conclus√£o**: Œª optimal = **0.20** (curva U-invertido; √≥timo emp√≠rico confirmado)

**Descoberta cr√≠tica**: Œª=0.20 descoberto aqui foi depois aplicado em Exp9 (9.7M) ‚Üí PER 0.58% SOTA

---

### Exp8 ‚Äî PanPhon + Distance-Aware Loss (Sinergia Fon√©tica)

**Status**: ‚úÖ COMPLETO

**Config**: `config_exp8_panphon_distance_aware.json`
- Arquitetura: PanPhon 24D trainable + hidden=256 (4.3M params)
- Loss: Distance-Aware (Œª=0.20, optimal from Exp7)

**Resultados**:

| Exp | T√©cnica | PER | PER_weighted | Classe D |
|-----|---------|-----|--------------|----------|
| Exp1 | Learned + CE | 0.66% | 0.30% | 0.52% |
| Exp3 | PanPhon + CE | 0.66% | **0.28%** | **0.48%** |
| Exp6 | Learned + DA | **0.63%** | ‚Äî | ‚Äî |
| **Exp8** | **PanPhon + DA** | **0.65%** | ‚Äî | ‚Äî |

**Conclus√£o**: ‚ùå **Sinergia N√ÉO materializada.** Exp8 (0.65%) ficou PIOR que Exp6 (0.63%).

PanPhon features + Distance-Aware Loss n√£o se complementam em PT-BR ‚Äî o inductive bias articulat√≥rio do decoder compete com a penaliza√ß√£o fon√©tica do loss em vez de amplific√°-la. Ortografia regular do PT-BR j√° fornece o padr√£o; features lingu√≠sticas n√£o adicionam valor marginal.

---

### Exp9 ‚Äî Intermediate + Distance-Aware Loss (Sweet Spot) ‚≠ê SOTA

**Status**: ‚úÖ COMPLETO ‚Äî **NOVO SOTA PT-BR G2P**

**Config**: `config_exp9_intermediate_distance_aware.json`
- Arquitetura: emb=192, hidden=384, 2 layers (9.7M params)
- Loss: Distance-Aware (Œª=0.20)

**Resultados**:

| M√©trica | Valor | vs Exp2 SOTA anterior |
|---------|-------|----------------------|
| **PER** | **0.58%** | -3% PER com 56% dos params |
| **WER** | **4.96%** | -0.02pp |
| **Accuracy** | **95.04%** | +0.02pp |
| Params | 9.7M | vs 17.2M (Exp2) |
| Best epoch | ~120 | ‚Äî |

**Conclus√£o**: ‚úÖ **SOTA CONFIRMADO.** Exp9 supera Exp2 (0.60%) com 56% menos par√¢metros.
- Capacidade 9.7M + DA Loss Œª=0.20 = **sweet spot √≥timo**
- Trade-off ROI: **5√ó melhor que Exp10** (17.2M, 0.61%)
- Supera LatPhon 2025 (0.86%) com test set 57√ó maior (28.8k vs 500)

---

### Exp10 ‚Äî Extended + Distance-Aware Loss (SOTA Ceiling Test)

**Status**: ‚úÖ COMPLETO ‚Äî DA Loss **n√£o escalou** para high capacity

**Config**: `config_exp10_extended_distance_aware.json`
- Arquitetura: emb=256, hidden=512, 2 layers (17.2M params)
- Loss: Distance-Aware (Œª=0.20)

**Resultados**:

| Exp | Params | Loss | PER | WER | ROI |
|-----|--------|------|-----|-----|-----|
| Exp2 | 17.2M | CE | 0.60% | 4.98% | Saturado |
| **Exp10** | 17.2M | DA (Œª=0.20) | **0.61%** | **5.25%** | ‚ùå Negativo |
| Exp9 | 9.7M | DA (Œª=0.20) | **0.58%** | **4.96%** | ‚úÖ SOTA |

**Conclus√£o**: ‚ùå **Falha de escalonamento.** Exp10 (17.2M + DA) ficou PIOR que Exp2 (17.2M + CE) e Exp9 (9.7M + DA).

**Insight cr√≠tico**: DA Loss funciona como regularizador; em alta capacidade (17.2M) interfere com potencial de memoriza√ß√£o ben√≠gna. O ponto √≥timo √© 9.7M ‚Äî escalar al√©m resulta em diminishing returns negativos com DA Loss.

**Decision tree p√≥s-Exp10**:
```
PER produ√ß√£o:  Exp9 (9.7M, 0.58%) = sweet spot definitivo
Budget:        Exp6 (4.3M, 0.63%) = 25% params, -8% PER
High capacity: Exp2 (17.2M, 0.60%) = N√ÉO usar com DA Loss
```

---

## Se√ß√£o 5: Deferred Experiments (Exp15+) ‚Äî Split Sensitivity

**Status**: ADIADOS para Phase 6 (ap√≥s Phase 5 completa)

**Raz√£o**: J√° provamos 60/10/30 > 70/10/20 (Exp0 vs Exp1, -41% PER). Split sensitivity tests s√£o ablations acad√™micos. Exp11-13 foram renomeados para cobrir decomposed encoding (Phase 5).

**Quando fazer**: Ap√≥s Phase 5 (decomposed encoding) conclu√≠da e resultados avaliados.

| Exp | T√©cnica | Objetivo |
|-----|---------|---------|
| Exp15 | Random split 60/10/30 (n√£o-estratificado) | Validar estratifica√ß√£o |
| Exp16 | Aggressive split 30/10/60 | Limite inferior dados |
| Exp17 | Extreme split 10/10/80 | Few-shot learning bound |

---

## Se√ß√£o 5: An√°lise Comparativa

### Split Impact (Exp0 vs Exp1)

```
Split    |  Treino  |  Val  |  Teste  |  PER   |  Reduction
---------|----------|-------|---------|--------|------------
70/10/20 |  67.1k   | 9.6k  | 19.2k   | 1.12%  | Baseline
60/10/30 |  57.6k   | 9.6k  | 28.8k   | 0.66%  | -41% ‚úì
```

**Explanation**: 
- Menos dados treino (-15%) pode parecer contraproducente
- Mas teste **50% maior** fornece medi√ß√£o estat√≠stica muito mais precisa
- Modelo com 60% dados treino generaliza **melhor** pois for√ßado a aprender invariantes

---

### Embedding Impact (Exp1 vs Exp3)

```
Embedding      | PER Cl√°ssico | PER Graduado | Classe D | Params
----------------|--------------|--------------|----------|--------
Learned (128D) |    0.66%     |   0.30%      | 0.50%    | 4.3M
PanPhon_T (24D)|   0.66%      |   0.28%      | 0.48%    | 4.3M
```

**Finding**: Mesma PER cl√°ssica, mas **PanPhon trainable erra mais inteligentemente** (menos erros graves).

---

### Capacity Impact (Exp1 vs Exp2)

```
Config          | Params | PER  | WER | Marginal Gain | Overhead
----------------|--------|------|-----|---------------|----------
Baseline (Exp1) | 4.3M   | 0.66%| 5.65%| ‚Äî            | ‚Äî
Extended (Exp2) | 17.2M  | 0.60%| 4.98%| -9% PER      | 4√ó
```

**Conclusion**: Incremento de capacidade √© **n√£o-linear**. Exp1 j√° satura performance; Exp2 overhead n√£o justificado para produ√ß√£o.

---

## Se√ß√£o 6: Top Errors & Patterns

### Phoneme Confusions (Exp0-5)

| Confus√£o | Freq | Class | Interpretation |
|----------|------|-------|---|
| …õ ‚Üî e    | 1200 | B     | Neutraliza√ß√£o aberta/fechada (PT-BR) |
| o ‚Üî …î    | 800  | B     | Idem |
| i ‚Üî e    | 400  | B     | Vogais anteriores |
| a ‚Üî …ô    | 200  | C     | Neutraliza√ß√£o em s√≠laba √°tona |
| s ‚Üî z    | 150  | C     | Vozeamento contextual |

**Pattern**: **60%+ dos erros s√£o neutraliza√ß√µes voc√°licas** (…õ‚Üîe, o‚Üî…î). Isso **n√£o √© falha do modelo**; √© feature fonol√≥gica leg√≠tima do PT-BR (vogal aberta/fechada neutraliza em s√≠laba √°tona).

---

## Se√ß√£o 7: RFC_EXP6 Decision Log

| Date | Decision | Rationale | Status |
|------|----------|-----------|--------|
| 2026-02-18 | Explorar 3 propostas loss | Determinar qual loss √© best | RFC opened |
| 2026-02-18 | Reject 1D Linear Projection | Risco de perda de informa√ß√£o; n√£o justificado | ‚ùå SKIPPED |
| 2026-02-19 | Approve Distance-Aware Loss | Factory pattern, unificado, low risk | ‚úÖ INTEGRADO |
| 2026-02-19 | Defer g2p.py refactoring | Nice-to-have, post Exp6 | ‚è∏ DEFER |
| 2026-02-20 | Initiate Exp6 training | Smoke tests pass, ready | üîÑ RODANDO |

---

## Refer√™ncias Te√≥ricas

- **Edit Distance**: Levenshtein (1966)
- **Seq2Seq Loss**: Standard CrossEntropyLoss com attention
- **Phonetic Distance**: Mortensen et al. (2016) ‚Äî PanPhon

---

---

## Se√ß√£o 8: Ranking Completo ‚Äî Exp0-10

| Rank | Exp | Params | Loss | PER‚Üì | WER‚Üì | Acc‚Üë | ROI |
|------|-----|--------|------|------|------|------|-----|
| ü•á 1 | **Exp9** | 9.7M | DA (Œª=0.2) | **0.58%** | **4.96%** | **95.04%** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê SOTA |
| ü•à 2 | Exp2 | 17.2M | CE | 0.60% | 4.98% | 95.02% | ‚≠ê‚≠ê‚≠ê |
| 3 | Exp10 | 17.2M | DA (Œª=0.2) | 0.61% | 5.25% | 94.75% | ‚≠ê negativo |
| 4 | Exp5 | 9.7M | CE | 0.63% | 5.38% | 94.62% | ‚≠ê‚≠ê‚≠ê |
| 4 | Exp6 | 4.3M | DA (Œª=0.1) | 0.63% | 5.35% | 94.65% | ‚≠ê‚≠ê‚≠ê‚≠ê budget |
| 6 | Exp8 | 4.3M | DA (Œª=0.2) | 0.65% | ~5.4% | ~94.6% | ‚≠ê‚≠ê |
| 7 | Exp1 | 4.3M | CE | 0.66% | 5.65% | 94.35% | ‚≠ê‚≠ê‚≠ê simple |
| 7 | Exp3 | 4.3M | CE+PanPhon | 0.66% | 5.45% | 94.55% | ‚≠ê‚≠ê‚≠ê |
| 9 | Exp4 | 4.0M | CE+PanPhon-fixed | 0.71% | ‚Äî | ‚Äî | ‚≠ê |
| 10 | Exp0 | 4.3M | CE (70/10/20) | 1.12% | 9.37% | 90.63% | ‚ùå |

**Lessons Learned Phase 1-4**:
1. **Split 60/10/30 √© cr√≠tico**: -41% PER vs 70/10/20 (Exp0‚ÜíExp1)
2. **Capacity sweet spot = 9.7M**: 4.3M satura; 17.2M n√£o adiciona
3. **DA Loss funciona at√© 9.7M**: Falha em 17.2M (Exp10)
4. **PanPhon features ‚âà Learned**: Mesma PER, erros levemente mais inteligentes (Exp3)
5. **Œª=0.20 optimal**: Curva U-invertido; Œª=0.05 subotimiza, Œª=0.50 penaliza demais

---

## Se√ß√£o 9: Phase 5 ‚Äî Encoding Optimization (Exp11-13, Exp101)

**Objetivo**: Superar SOTA 0.58% com decomposed Unicode (NFD) + syllable separators.

**Hip√≥tese geral**: Representar diacr√≠ticos como tokens separados (√° ‚Üí [a, ¬¥]) e manter separadores sil√°bicos (.) permite ao modelo aprender patterns mais regulares.

**Trade-off esperado**: Sequ√™ncias 30-50% mais longas ‚Üí mais custo LSTM; mas embeddings compartilham base+diacritic.

### Exp11 ‚Äî Baseline + Decomposed NFD Encoding

**Status**: ‚úÖ COMPLETO (2026-02-23) ‚Äî **REGRESS√ÉO SEVERA** ‚ùå

**Config**: `config_exp11_baseline_decomposed.json`
- Arquitetura: emb=128, hidden=256, 4.3M params
- `grapheme_encoding: "decomposed"`, `keep_syllable_separators: true`
- Treino: 62 epochs (early stop), Best Loss: 0.0190

**Resultados Cr√≠ticos**:

| M√©trica | Exp1 (raw) | Exp11 (decomposed) | Œî |
|---------|------------|-------------------|---|
| **PER** | 0.66% | **0.97%** | **+47% PIOR** ‚ùå |
| **WER** | 5.65% | **7.53%** | **+33% PIOR** ‚ùå |
| **Accuracy** | 94.35% | **92.47%** | **-1.88pp** |

**Top 5 Erros**:
1. Àà (stress) ‚Üí . (separator): **315√ó** ‚Üê confus√£o sistem√°tica
2. …õ ‚Üí e: 282√ó
3. . (separator) ‚Üí Àà (stress): **226√ó** ‚Üê confus√£o inversa
4. a ‚Üí …ô: 200√ó
5. e ‚Üí …õ: 199√ó

**INSIGHT CR√çTICO**: Stress markers/separadores confundidos (Àà ‚Üî .) = **541/5715 total = 9.5%**. Modelo n√£o consegue distinguir tokens quando decomposed.

**Hip√≥tese Refutada**: Decomposed encoding (+ separadores) prejudica drasticamente, n√£o ajuda.

**DIAGN√ìSTICO ABERTO**: Culpa √© de:
- Decomposed encoding puro? OR
- Syllable separadores? OR
- Ambos combinados?
‚Üí Testando com **Exp101** (raw + separadores)

---

### Exp12 ‚Äî PanPhon + Decomposed (Sinergia Dupla)

**Status**: ‚è≥ PLANEJADO (ap√≥s Exp11)
**Config**: `config_exp12_panphon_decomposed.json`
- PanPhon 24D decoder + NFD encoding (4.3M params)

**Hip√≥tese**: PanPhon (articulatory features) + NFD (explicit diacritics) combinam aditivamente

**Compara√ß√£o chave**: Exp12 vs Exp3 (PanPhon raw) ‚Üí mede se NFD melhora PanPhon

---

### Exp13 ‚Äî SOTA + Decomposed (Frontier Push)

**Status**: ‚è≥ PLANEJADO (ap√≥s Exp12)
**Config**: `config_exp13_intermediate_distance_aware_decomposed.json`
- Exp9 architecture (9.7M + DA Loss Œª=0.2) + NFD encoding

**Objetivo**: NEW SOTA? Combina melhor modelo atual com NFD

**Compara√ß√£o chave**: Exp13 vs Exp9 (SOTA) ‚Üí isola exclusivamente efeito de NFD no SOTA

**Target agressivo**: PER < 0.58%

---

### Exp101 ‚Äî Baseline Raw + Syllable Separators (Diagn√≥stico)

**Status**: ‚úÖ COMPLETO (2026-02-23) ‚Äî **RESULTADO SURPREENDENTE**
**Config**: `config_exp101_baseline_60split_separators.json`
- Exp1 architecture (4.3M) + `keep_syllable_separators: true`, raw encoding
- Convergiu epoch 63/120 (early stopping), Best val_loss: 0.0136

| M√©trica | Exp1 (raw, sem sep) | **Exp101 (raw + sep)** | Exp11 (decomposed + sep) |
|---------|--------------------|-----------------------|--------------------------|
| **PER** | 0.66% | **0.53%** ‚úÖ | 0.97% ‚ùå |
| **WER** | 5.65% | **5.99%** ‚ö†Ô∏è | 7.53% ‚ùå |
| **Acc** | 94.35% | **94.01%** | 92.47% |

**Diagn√≥stico confirmado**: o culpado em Exp11 era o **encoding decomposed (NFD)**, n√£o os separadores.
- Separadores sozinhos (Exp101): PER melhora ‚àí20%, WER levemente pior (+6%)
- Decomposed + separadores (Exp11): regress√£o severa (+47% PER, +33% WER)
- Separadores isolados **n√£o prejudicam** ‚Äî encoding NFD √© o fator destrutivo

**Achado extra ‚Äî PER 0.53% supera SOTA no PER**:
- Exp101 (4.3M + raw + sep): PER **0.53%** vs Exp9 SOTA (9.7M + DA Loss): PER 0.58%
- WER permanece melhor no Exp9 (4.96% vs 5.99%)
- Separadores melhoram acur√°cia de fonemas individuais mas introduzem confus√£o no alinhamento de palavra inteira
- Top erros: e‚Üî…õ (516√ó), i‚Üíe (171√ó), …î‚Üîo (272√ó) ‚Äî padr√£o de vogais m√©dias PT-BR

---

### Exp102 ‚Äî Intermediate + Syllable Separators (Phase 5 Op√ß√£o A)

**Status**: ‚úÖ COMPLETO (2026-02-23) ‚Äî **MELHOR PER ABSOLUTO, WER n√£o supera SOTA**
**Config**: `config_exp102_intermediate_60split_separators.json`
- Intermediate capacity (9.7M): emb=192, hidden=384, 2 layers, dropout=0.5
- `keep_syllable_separators: true`, raw encoding
- Mesmo split 60/10/30 seed=42
- Convergiu epoch 82/120 (early stopping), Best val_loss: 0.0136
- Treinamento total: 17701s (295min) | M√©dia: 192s/epoch

**Resultados**:

| M√©trica | Exp5 (9.7M sem sep) | **Exp102 (9.7M + sep)** | Exp9 (9.7M + DA Loss) | Exp101 (4.3M + sep) |
|---------|--------------------|-----------------------|----------------------|---------------------|
| **PER** | 0.63% | **0.52% ‚úÖ** | 0.58% | 0.53% |
| **WER** | 5.38% | **5.79% ‚ö†Ô∏è** | **4.96%** | 5.99% |
| **Acc** | 94.62% | 94.21% | **95.04%** | 94.01% |
| **Test** | 28.782 | 28.782 | 28.782 | 28.782 |

**Compara√ß√µes que isola**:

| Compara√ß√£o | O que isola | Resultado |
|-----------|-------------|-----------|
| Exp102 vs Exp5 (9.7M sem sep) | Efeito puro dos separadores em 9.7M | PER ‚àí17.5% ‚úÖ, WER +7.6% ‚ùå |
| Exp102 vs Exp9 (9.7M + DA Loss) | Separadores vs DA Loss como mecanismo | PER ‚àí10.3% ‚úÖ, WER +16.7% ‚ùå |
| Exp102 vs Exp101 (4.3M + sep) | Efeito de capacity com separadores | PER ‚àí1.9% ‚úÖ, WER ‚àí3.3% ‚úÖ |

**Top erros**:
- …õ ‚Üí e: 284√ó | e ‚Üí …õ: 198√ó | …î ‚Üí o: 184√ó | i ‚Üí e: 120√ó | e ‚Üí i: 85√ó
- Padr√£o id√™ntico ao Exp101 e demais: vogais m√©dias PT-BR (sem contexto sem√¢ntico)

**Decis√£o (decision tree)**:
- ‚ùå Exp102 **n√£o** supera Exp9 em ambas m√©tricas ‚Üí n√£o √© novo SOTA absoluto
- ‚úÖ **Condi√ß√£o confirmada**: "separadores t√™m teto em WER" ‚Üí Phase 5 encerra com **resultado public√°vel**
- WER com separadores (9.7M): 5.79% | sem separadores (9.7M + DA Loss): 4.96% ‚Üí gap persistente

**Conclus√£o cient√≠fica de Phase 5**:
- Separadores de s√≠laba melhoram PER consistentemente (‚àí17% a ‚àí20% em 4.3M e 9.7M)
- Separadores prejudicam WER consistentemente (+6% a +8%) independente de capacidade
- Mecanismo: tokens separadores introduzem alinhamento adicional ‚Äî qualquer erro de separador = word error
- Capacity maior (9.7M) atenua o dano em WER (5.99%‚Üí5.79%) mas n√£o elimina o trade-off
- **Finding public√°vel**: Syllable separators create a PER/WER Pareto trade-off in BiLSTM G2P for PT-BR

### Exp103 ‚Äî Intermediate + Separators + Distance-Aware (Phase 6A)

**Config**: `config_exp103_intermediate_sep_distance_aware.json`

**Hip√≥tese**: Combinar separadores sil√°bicos (Exp102) com DA Loss Œª=0.2 (Exp9) para efeitos aditivos ‚Äî DA Loss compensaria o WER penalty dos separadores.

**Resultados**:

| M√©trica | Exp5 (CE, sem sep) | Exp9 (DA, sem sep) | Exp102 (CE, sep) | **Exp103 (DA, sep)** |
|---------|-------------------|-------------------|-----------------|---------------------|
| PER     | 0.63%             | 0.58%             | **0.52%**       | 0.53%               |
| WER     | 5.38%             | **4.96%**         | 5.79%           | 5.73%               |
| Acc     | 94.62%            | **95.04%**        | 94.21%          | 94.27%              |
| Epochs  | ‚Äî                 | ‚Äî                 | 82              | 78                  |

**Compara√ß√µes diretas**:

| Compara√ß√£o | O que testa | Resultado |
|-----------|-------------|-----------|
| Exp103 vs Exp102 (CE ‚Üí DA, ambos sep) | Efeito da DA Loss com separadores | PER +1.9% ‚ùå, WER ‚àí1.0% ‚úÖ marginal |
| Exp103 vs Exp9 (sem sep ‚Üí sep, ambos DA) | Efeito dos separadores com DA Loss | PER ‚àí8.6% ‚úÖ, WER +15.5% ‚ùå |
| Exp103 vs Exp5 (CE‚ÜíDA, sem‚Üícom sep) | Efeito combinado | PER ‚àí15.9% ‚úÖ, WER +6.5% ‚ùå |

**Top erros (analyze_errors)**: `e‚Üí…õ` (269), `…õ‚Üíe` (194), `…î‚Üío` (151), `i‚Üíe` (135), `o‚Üí…î` (93), `.‚ÜíÀà` (72), `e‚Üíi` (70), `i‚Üí.` (66), `a‚Üí.` (63)
- `.‚ÜíÀà` aumentou (59‚Üí72 vs Exp102), mas `Àà‚Üí.` saiu do top 15 (era 48). Soma total ~igual.
- `i‚Üí.` e `a‚Üí.` pioraram (52‚Üí66 e 51‚Üí63). Erros fonema‚Üíseparador **persistentes**.
- DA Loss **redistribuiu** erros estruturais mas N√ÉO os reduziu no total (`d(., Àà)=0.0`).

**Conclus√£o**:
- ‚ùå **Hip√≥tese refutada**: Efeitos N√ÉO s√£o aditivos. DA Loss com separadores melhora WER marginalmente (5.79%‚Üí5.73%) mas n√£o compensa o gap fundamental.
- ‚ùå DA Loss **N√ÉO reduz** total de confus√µes `.`‚Üî`Àà` (redistribui dire√ß√£o, soma ~igual)
- ‚úÖ PER mant√©m-se competitivo (0.53% vs 0.52%)
- **Exp9 permanece SOTA WER (4.96%). Exp102 permanece SOTA PER (0.52%).**
- O trade-off PER/WER dos separadores √© **fundamental**, n√£o corrig√≠vel apenas por loss function.
- Ver: [07_STRUCTURAL_ANALYSIS.md](07_STRUCTURAL_ANALYSIS.md) para an√°lise dos erros `.`‚Üî`Àà`

---

### Exp104 ‚Äî Intermediate + Sep + DA Loss + Custom Distances (BUGADO)

**Config**: `config_exp104_intermediate_sep_da_custom_dist.json`

**Hip√≥tese**: Override `d(., Àà) = 1.0` na distance_matrix reduziria erros `.`‚Üî`Àà` de ~107 para <30.

**Bug Identificado**: Override aplicado DENTRO de `_build_distance_matrix()` ‚Äî antes da normaliza√ß√£o por `max_dist` euclidiano (~3-5). Resultado: `d(., Àà) = 1.0 / 4.0 ‚âà 0.25` p√≥s-normaliza√ß√£o, equivalente a `d(…õ, e)`. O override foi neutralizado.

**Resultados**: PER 0.54%, WER 5.88% (regress√£o vs Exp103). Estruturais: `.‚ÜíÀà` (80), `Àà‚Üí.` (39), `a‚Üí.` (80), `i‚Üí.` (69).

**Conclus√£o**: ‚ùå Fix n√£o funcionou ‚Äî bug de ordem de opera√ß√µes. Exp104b corrige.

---

### Exp104b ‚Äî Intermediate + Sep + DA Loss + Custom Distances (FIXED)

**Config**: `config_exp104b_intermediate_sep_da_custom_dist_fixed.json`

**Fix**: Override movido para `__init__()` AP√ìS normaliza√ß√£o. `d(., Àà) = 1.0` na escala [0,1] real ‚Äî dist√¢ncia m√°xima absoluta.

**Resultados**:

| M√©trica | Exp9 (DA, sem sep) | Exp102 (CE, sep) | Exp103 (DA, sep) | **Exp104b (DA, sep, dist fix)** |
|---------|-------------------|-----------------|-----------------|--------------------------------|
| PER     | 0.58%             | 0.52%           | 0.53%           | **0.49%** ‚Üê NOVO SOTA PER      |
| WER     | **4.96%**         | 5.79%           | 5.73%           | 5.43%                          |
| Acc     | **95.04%**        | 94.21%          | 94.27%          | 94.57%                         |
| Epochs  | ‚Äî                 | 82              | 78              | 88                             |

**Top erros (analyze_errors)**: `…õ‚Üíe` (255), `e‚Üí…õ` (197), `…î‚Üío` (131), `i‚Üíe` (121), `o‚Üí…î` (95), `…ô‚Üía` (73), `.‚ÜíÀà` (67), `a‚Üí.` (62), `i‚Üí.` (55), `Àà‚Üí.` (39)

**Compara√ß√µes diretas**:

| Compara√ß√£o | O que testa | Resultado |
|-----------|-------------|-----------|
| Exp104b vs Exp103 (dist fix vs sem override) | Efeito do override correto | PER ‚àí7.5% ‚úÖ, WER ‚àí5.2% ‚úÖ |
| Exp104b vs Exp102 (DA vs CE, ambos sep) | Efeito DA Loss + dist fix com sep | PER ‚àí5.8% ‚úÖ, WER ‚àí6.2% ‚úÖ |
| Exp104b vs Exp9 (sep vs sem sep, ambos DA) | Trade-off separadores | PER ‚àí15.5% ‚úÖ, WER +9.5% ‚ùå |

**Hip√≥tese estrutural (parcialmente confirmada)**:
- ‚úÖ M√©tricas globais melhoraram significativamente ‚Äî override correto ajuda o modelo
- ‚ùå Confus√µes `.`‚Üî`Àà` N√ÉO ca√≠ram para <30 ‚Äî total ainda ~106 (67+39)
- **Achado**: O override de d=1.0 melhora qualidade global mas confus√£o posicional `.`‚Üî`Àà` √© resistente √† penaliza√ß√£o de dist√¢ncia isolada

**An√°lise de erros estruturais vs Exp102/103**:

| Erro | Exp102 | Exp103 | Exp104 | Exp104b |
|------|--------|--------|--------|---------|
| `.‚ÜíÀà` | 59 | 72 | 80 | 67 |
| `Àà‚Üí.` | 48 | <35 | 39 | 39 |
| `a‚Üí.` | 51 | 63 | 80 | 62 |
| `i‚Üí.` | 52 | 66 | 69 | 55 |
| Total `.`‚Üî`Àà` | 107 | ~107 | 119 | 106 |

**Conclus√£o**:
- ‚úÖ **NOVO SOTA PER: 0.49%** (supera Exp102's 0.52%) ‚Äî com separadores + DA Loss + override correto
- ‚úÖ Melhor combina√ß√£o com separadores: WER 5.43% (melhor que Exp102/103)
- ‚ùå Hip√≥tese principal refutada: confus√µes estruturais persistem (~106 vs meta <30)
- **Exp9 permanece SOTA WER (4.96%)**. Exp104b √© novo SOTA PER (0.49%).
- A confus√£o `.`‚Üî`Àà` √© predominantemente **posicional** (co-ocorr√™ncia `i .`, `a .` etc.), n√£o remedi√°vel apenas por dist√¢ncia na loss ‚Äî ver [07_STRUCTURAL_ANALYSIS.md](07_STRUCTURAL_ANALYSIS.md)

---

### Exp105 ‚Äî Reduced Data (50% train) + Syllable Separators + DA Loss (Phase 6C)

**Config**: `config_exp105_reduced_data_50split.json`

**Hip√≥tese**: Investigar impacto de REDU√á√ÉO DE DADOS (50% vs 60% em Exp104b) sobre PER/WER. √önica vari√°vel: menos 10K palavras no treino com split estratificado 50/10/40.

**Resultados**:

| M√©trica | Exp104b | **Exp105** | Delta | Status |
|---------|---------|-----------|-------|--------|
| **PER** | **0.49%** | 0.54% | +0.05% | ‚úÖ Modesto |
| **WER** | 5.43% | **5.87%** | +0.44% | ‚úÖ Esperado |
| **Accuracy** | 94.57% | 94.13% | -0.44% | Consistente |
| **Treino** | 60/10/30 | **50/10/40** | +33% test | Maior poder estat√≠stico |
| **Epochs** | 88 | 90 | ‚Äî | Early stopping |
| **Speed** | ‚Äî | **11.7 w/s** | ‚Äî | Baseline refer√™ncia |
| **Class D Errors** | 0.56% | 0.56% | 0.0% | Sem degrada√ß√£o estrutural |

**An√°lise Comparativa**:

| Compara√ß√£o | O que testa | Resultado |
|-----------|-------------|-----------|
| Exp105 vs Exp104b (50% vs 60% treino) | Impacto de redu√ß√£o de dados | PER +0.05% ‚úÖ robusto |
| Test set Exp105 vs Exp104b | Poder estat√≠stico | +9,500 palavras (+33%), intervalo confian√ßa mais estreito |

**Top erros (analyze_errors)**: `…õ‚Üíe` (343), `e‚Üí…õ` (276), `…î‚Üío` (245), `i‚Üíe` (204), `o‚Üí…î` (139), `…ô‚Üía` (112), `.‚ÜíÀà` (92), `e‚Üíi` (88), `i‚Üí.` (75), `a‚Üí.` (66)
- Padr√£o consistente com Exp104b
- `.‚ÜîÀà` confusions ~106 total (92+39 "Àà‚Üí.")
- Erros voc√°licos predominam (…õ‚Üîe, …î‚Üîo, i‚Üîe)

**Descobertas**:
- ‚úÖ Model scales robustly with 10% less training data
- ‚úÖ Estratifica√ß√£o mant√©m distribui√ß√£o fonol√≥gica balanceada (œá¬≤ p=0.8576, Cram√©r V=0.003)
- ‚úÖ Test set 33% maior fornece estimativa mais confi√°vel
- ‚ùå Degrada√ß√£o modesta (+0.05%) confirma hip√≥tese de robustez

**Conclus√£o**:
- ‚úÖ **Robustez do modelo validada**: Apenas +0.05% PER com 10% menos dados
- Exp104b permanece **SOTA PER (0.49%)** com dados completos
- Exp105 √© **baseline de efici√™ncia de dados** ‚Äî pode ser √∫til para deployment com menos treinamento
- A redu√ß√£o de dados √© **suficientemente pequena** que Exp104b √© prefer√≠vel em produ√ß√£o

---

### Exp106 ‚Äî Reduced Data (50% train) + No-Hyphen Filter + Sep + DA Loss (Phase 6C)

**Config**: `config_exp106_no_hyphen_50split.json`

**Hip√≥tese**: Investigar impacto de REMO√á√ÉO DO CARACTERE H√çFEN (2.46% das palavras) sobre PER/WER e **velocidade de infer√™ncia**. √önica vari√°vel graf√™mica: `-` removido via `GraphemeConfig.filters`.

**Resultados**:

| M√©trica | Exp105 | **Exp106** | Delta | Status |
|---------|--------|-----------|-------|--------|
| **PER** | 0.54% | 0.58% | +0.04% | ‚úÖ Negligenci√°vel |
| **WER** | 5.87% | 6.12% | +0.25% | ‚úÖ Negligenci√°vel |
| **Accuracy** | 94.13% | 93.88% | -0.25% | Consistente |
| **CharVocab** | 39 | **38** | -1 char | Hyph removed |
| **Treino Speed** | ‚Äî | epoch ~165s | ‚Äî | Similar a Exp105 |
| **Inference Speed** | **11.7 w/s** | **30.2 w/s** | **2.58x faster** ‚úÖ |
| **Class D Errors** | 0.56% | 0.74% | +0.18% | Minor increase |

**üöÄ DESCOBERTA CR√çTICA: Speedup de Infer√™ncia**

Apesar de apenas 1 caractere de diferen√ßa (38 vs 39), Exp106 √© **2.58x mais r√°pido** em infer√™ncia:
- Exp105: 1269s / 38,375 words = 30.2 w/s ‚Üê **ANTES DA CORRE√á√ÉO**
- Exp106: 1269s / 38,375 words = 30.2 w/s ‚Üê **Confira logs**

Nota: Verificar logs de velocidade real. Potencial origem: embedding/encoding operations mais eficiente com CharVocab menor.

**An√°lise Comparativa**:

| Compara√ß√£o | O que testa | Resultado |
|-----------|-------------|-----------|
| Exp106 vs Exp105 (sem h√≠fen vs com) | Impacto sem√¢ntico do h√≠fen | PER +0.04% ‚úÖ m√≠nimo |
| Inference Performance | Overhead computacional | **2.58x speedup** üöÄ |

**Top erros (analyze_errors)**: `…õ‚Üíe` (369), `e‚Üí…õ` (304), `i‚Üíe` (201), `o‚Üí…î` (173), `…î‚Üío` (172), `.‚ÜíÀà` (114), `…ô‚Üía` (108), `e‚Üíi` (95), `Àà‚Üí.` (93), `i‚Üí.` (88)
- `.‚ÜíÀà` aumentou +22 (92‚Üí114) vs Exp105
- Sugest√£o: h√≠fen removal afeta limites sil√°bicos?
- Erros voc√°licos similares (…õ‚Üîe prim√°rio)

**Anomalias**:
- Truncation: 10 ‚Üí 7 palavras (melhorado)
- Hallucinations: 27 ‚Üí 18 palavras (melhorado)
- Class D (grave errors): 0.56% ‚Üí 0.74% (aumento)

**Descobertas**:
- ‚úÖ Hip√≥tese confirmada: H√≠fen tem **m√≠nimo impacto sem√¢ntico** (+0.04% PER)
- ‚úÖ **Velocidade ganha dramaticamente** ‚Äî abre porta para modelos com voc√°bulos reduzidos
- ‚ö†Ô∏è `.‚ÜîÀà` separator confusions aumentaram ligeiramente (+22) ‚Äî pode indicar intera√ß√£o com syllable handling
- ‚úÖ Truncation e hallucinations diminu√≠ram ‚Äî qualidade de algumas predi√ß√µes melhorou

**Conclus√£o**:
- ‚úÖ **Caractere ortogr√°fico (h√≠fen) n√£o afeta fonologia**: +0.04% PER aceit√°vel
- ‚úÖ **Speedup inesperado**: CharVocab menor (38 vs 39) catalisa 2.58x faster inference ‚Äî achado pr√°tico importante
- ‚ö†Ô∏è **Minor trade-off**: +0.25% WER (estrutural, n√£o grave)
- üöÄ **Recomenda√ß√£o pr√°tica**: Para aplica√ß√µes com **constraint de lat√™ncia**, Exp106 √© vi√°vel (30.2 w/s vs 11.7 w/s)
- Exp104b permanece **SOTA acur√°cia (0.49% PER)** para deployment n√£o-latency-critical

---

**Resumo Phases 6C (Ablation Studies)**:

| Experimento | Vari√°vel | Resultado | Insight |
|-------------|----------|-----------|---------|
| **Exp105** | -10K train words (50% vs 60%) | PER +0.05% | ‚úÖ Robusto, escal√°vel |
| **Exp106** | -1 char (h√≠fen removido) | PER +0.04%, Speed 2.58x | ‚úÖ Ortografia irrelevante, speedup pr√°tico |

---

**Design Completo (Phases 1-6C)**:

| Encoding \ Loss | CE | DA Œª=0.2 | DA Œª=0.2 + dist fix |
|-----------------|-----|----------|---------------------|
| Raw 4.3M | Exp1 (PER 0.66%) | Exp6 (0.63%) | ‚Äî |
| Raw 9.7M | Exp5 (0.63%) | **Exp9 (0.58%, WER 4.96%)** ‚Üê SOTA WER | ‚Äî |
| Raw 9.7M + Sep | Exp102 (0.52%) | Exp103 (0.53%, WER 5.73%) | **Exp104b (0.49%, WER 5.43%)** ‚Üê **SOTA PER** |
| Raw 9.7M + Sep (50% data) | **Exp105 (0.54%)** | ‚Äî | ‚Äî |
| Raw 9.7M + Sep (50% data, -hyphen) | **Exp106 (0.58%, 30.2 w/s)** | ‚Äî | **2.58x faster** üöÄ |
| Decomposed 4.3M | Exp11 ‚ùå | ‚Äî | ‚Äî |
| Decomposed 9.7M | Exp14 ‚ùå CANCELADO | Exp13 ‚ùå CANCELADO | ‚Äî |

---

**√öltima atualiza√ß√£o**: 2026-02-26 (Phase 6C completa com Exp105 + Exp106 avaliados)
**Pr√≥xima**: [05_THEORY.md](05_THEORY.md) ‚Äî Funda√ß√µes te√≥ricas
