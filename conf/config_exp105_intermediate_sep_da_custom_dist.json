{
  "description": "Exp105 — Intermediate 9.7M + Sep + DA Loss (λ=0.2) + Custom Dist + Split 50/10/40 (dados de treino reduzidos)",
  "data": {
    "source": "dicts/pt-br.tsv",
    "train_ratio": 0.5,
    "val_ratio": 0.1,
    "test_ratio": 0.4,
    "split_seed": 42,
    "note": "Split 50/10/40 para avaliar impacto de menos dados de treino; test maior (≈38.4K) dá mais poder estatístico. Corpus normalizado para NFC em 2026-02-25 (encoding uniforme, elimina tokens fantasma ẽ/ũ/õ NFD).",
    "grapheme_encoding": "raw",
    "keep_syllable_separators": true
  },
  "model": {
    "embedding_type": "learned",
    "emb_dim": 192,
    "hidden_dim": 384,
    "num_layers": 2,
    "dropout": 0.5,
    "note": "Arquitetura idêntica ao Exp104b (9.7M). Aumentar neurônios não é recomendado: Exp2 (17.2M) foi pior que Exp5 (9.7M) com 60% de treino; com 50% o risco de overfitting aumenta ainda mais. Ablação limpa: única variável é a quantidade de dados + corpus NFC.",
    "total_params_estimate": "~9.7M"
  },
  "training": {
    "fit_method": "adam",
    "epochs": 120,
    "warmup_epochs": 80,
    "batch_size": 64,
    "lr": 0.001,
    "early_stopping_patience": 10,
    "weight_decay": 1e-05,
    "loss": {
      "type": "distance_aware",
      "config": {
        "distance_lambda": 0.2,
        "distance_metric": "euclidean",
        "normalize_distance": true
      },
      "note": "λ=0.2 ótimo de Exp7. Override estrutural pós-normalização (fix do Exp104b). d(.,ˈ)=1.0 real na escala [0,1]."
    }
  },
  "experiment": {
    "name": "exp105_intermediate_sep_da_nfc_50split",
    "purpose": "Avaliar impacto de REDUÇÃO DE DADOS DE TREINO (50% vs 60% em Exp104b) sobre desempenho. Única variável isolada: menos 10K palavras de treino. Corpus normalizado para NFC é transparent (verificado em 12_DATA_PIPELINE.md).",
    "hypothesis": "Com 10% menos dados de treino (50% vs 60%), o PER pode aumentar marginalmente (0.49→0.50-0.55%), WER similar. Test set maior (≈38.4K vs ≈28.8K) aumenta confiança estatística. NFC normalization não afeta resultados (transparent).",
    "corpus_normalization": {
      "status": "TRANSPARENT AO PIPELINE",
      "change": "dicts/pt-br.tsv normalizado para Unicode NFC em 2026-02-25",
      "encoding_before": "Misto: NFD predominante + 10 entradas espúrias NFC",
      "encoding_after": "NFC uniforme em todo o arquivo",
      "impact": "NENHUM no comportamento do modelo. Motivo: g2p.py normaliza phonemes para NFC em _load_tsv() (linha 367). CharVocab, PhonemeVocab, splits são idênticos. Verificação: docs/12_DATA_PIPELINE.md § 12.6.",
      "phonological_discovery": "Regra ɣ/x confirmada: ɣ=r-coda antes C vozeada (b,d,n,m,v,z,ʒ,ɡ), x=r-coda final. Distribuição complementar perfeita em 5.449 entradas. Ver docs/11_CORPUS_AUDIT.md."
    },
    "split_rationale": {
      "train": "50% ≈ 47.968 palavras (vs 57.561 no Exp104b)",
      "val": "10% ≈ 9.594 palavras (idêntico)",
      "test": "40% ≈ 38.375 palavras (vs 28.782 — +33% poder estatístico)"
    },
    "architecture_decision": {
      "keep_9.7M": true,
      "reason": "Exp2 (17.2M) foi PIOR que Exp5 (9.7M) em mesmas condições — scaling não é limitante. Com 50% de treino (vs 60%), modelo maior = risco maior de overfitting. Ablação deve isolar UMA variável: aqui é dados de treino.",
      "hyphen_analysis": "Caractere '-' está em 2.46% das palavras (2.361/95.937). Overhead: negligenciável. Mantém: PT-BR representa fielmente (compostos, verbos+pronome). Não remover nesta ablação.",
      "future": "Se Exp105 < 0.50% PER: sucesso (tolerância 0.01%). Se 0.50-0.55%: mudança marginal esperada (menos dados). Se > 0.55%: investigar qual variável adicional mudou."
    },
    "compared_with": [
      "exp104b: 9.7M + sep + DA λ=0.2 + custom dist, split 60/10/30  (PER 0.49%, WER 5.43%) — SOTA PER, referência principal",
      "exp102:  9.7M + sep + CE,                      split 60/10/30  (PER 0.52%, WER 5.79%) — baseline sep",
      "exp9:    9.7M + DA λ=0.2 (sem sep),             split 60/10/30  (PER 0.58%, WER 4.96%) — SOTA WER",
      "exp1:    4.3M baseline,                          split 60/10/30  (PER 0.66%, WER 5.65%) — baseline",
      "exp2:    17.2M,                                  split 60/10/30  (PER 0.60%, WER 5.63%) — scaling não ajudou"
    ],
    "expected_metrics": {
      "per_estimate": "~0.49-0.55%",
      "wer_estimate": "~5.4-5.8%",
      "note": "Degradação esperada modesta por menos treino; corpus NFC pode compensar ruído de encoding. Test maior → intervalos de confiança mais estreitos."
    },
    "training_time_estimate": "~5-6h GPU (menos dados = épocas mais rápidas)",
    "total_params": "~9.7M",
    "risk": "Com 10K palavras a menos no treino, early stopping pode ativar mais cedo — monitorar se val_loss converge bem. Patience=10 adequado.",
    "notes": [
      "Phase 6C: ablação de split + corpus NFC",
      "Config criado: 2026-02-25",
      "Baseado em Exp104b (config_exp105_intermediate_sep_da_custom_dist.json renomeado/adaptado)",
      "Corpus NFC auditado: docs/11_CORPUS_AUDIT.md",
      "Descoberta ɣ/x: generalization_test.tsv corrigido (4 entradas: borboleta, computadorzinho, açucarzão, internet)"
    ]
  }
}
